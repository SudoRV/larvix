export const callOpenAI = async (message) => {
  const normalized = message.toLowerCase().trim();

  if (normalized.includes("backpropagation")) {
    return `
## Backpropagation

Backpropagation is a supervised learning algorithm used to train artificial neural networks. It updates the weights of the network by minimizing the loss function using gradient descent and the chain rule of calculus.

---

### ðŸ”¹ Forward Pass

For a neuron:

z = wÂ·x + b  
a = f(z)

Where:
- w = weights  
- x = input  
- b = bias  
- f(z) = activation function  
- a = output  

---

### ðŸ”¹ Loss Function

For Mean Squared Error (MSE):

L = (1/2)(y - Å·)^2

Where:
- y = actual value  
- Å· = predicted value  

---

### ðŸ”¹ Backward Pass

Using the chain rule:

âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚a)(âˆ‚a/âˆ‚z)(âˆ‚z/âˆ‚w)

For sigmoid activation:

Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))

Error term:

Î´ = (Å· - y) Ã— Ïƒ'(z)

---

### ðŸ”¹ Weight Update Rule

w_new = w_old - Î·(âˆ‚L/âˆ‚w)

Where:
- Î· = learning rate  

---

### ðŸ”¹ Summary

Backpropagation:
- Computes prediction error  
- Propagates error backward layer-by-layer  
- Updates weights iteratively  
- Minimizes loss efficiently  

It is the core optimization mechanism behind deep learning.
`;
  }

  return "Sorry, this demo only supports the question about backpropagation.";
};













// import OpenAI from "openai";

// const client = new OpenAI({
//   apiKey: process.env.OPENAI_API_KEY
// });

// export const callOpenAI = async (message) => {
//   const response = await client.chat.completions.create({
//     model: "gpt-4o-mini",
//     messages: [
//       { role: "user", content: message }
//     ]
//   });

//   return response.choices[0].message.content;
// };