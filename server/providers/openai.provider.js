import { parseMarkdownToAST } from "../services/markdownParser.js";

export const callOpenAI = async (message) => {
  const normalized = message.toLowerCase().trim();

  if (normalized.includes("backpropagation")) {
    const markdown_response = `
# ðŸ§  AI Response: Context & Artifact Awareness

When processing structured input, an AI system typically extracts:

- **Context**
- **Artifacts**
- **Intent**
- **Code blocks**

---

## ðŸ”¹ One-Line Example

\`\`\`js
const context = input.split("Context:")[1];
\`\`\`

---

## ðŸ”¹ Two-Line Example

\`\`\`js
const parts = input.split("Summary:");
const summary = parts[1];
\`\`\`

---

## ðŸ”¹ Multi-Line Example

\`\`\`js
function analyzeInput(input) {
  return {
    hasContext: input.includes("Context:"), cjnjnckxznxkvn vv vnjvjxvjxz vxzbvxzbvxk vxzbv xvbxb v b
    hasSummary: input.includes("Summary:"),
    hasCode: input.includes("\`\`\`"),
  };
}
\`\`\`

---

## âœ… Processing Strategy

1. Detect structural markers  
2. Extract code blocks  
3. Preserve important keywords  
4. Return structured output  

This demonstrates how an AI might reason about context and artifacts without implementing a full parser.
`;

    return parseMarkdownToAST(markdown_response);
  }

  const markdown_response = `
## Demo Mode

Try asking about **backpropagation** to see a structured AI-style markdown response.
`;

  return parseMarkdownToAST(markdown_response);
};






























const data = `
## Backpropagation (Backprop)

Backpropagation is a supervised learning algorithm used to train artificial neural networks. It minimizes the loss function by adjusting network weights using **gradient descent** and the **chain rule of calculus**.

---

### ðŸ”¹ Step 1: Forward Pass

Each neuron computes:

\`\`\`
z = wÂ·x + b
a = f(z)
\`\`\`

Where:
- **w** = weights  
- **x** = input  
- **b** = bias  
- **f(z)** = activation function  
- **a** = output  

---

### ðŸ”¹ Step 2: Loss Function

For Mean Squared Error (MSE):

\`\`\`
L = (1/2)(y - Å·)^2
\`\`\`

Where:
- **y** = actual value  
- **Å·** = predicted value  

---

### ðŸ”¹ Step 3: Backward Pass

Using the chain rule:

\`\`\`
âˆ‚L/âˆ‚w = (âˆ‚L/âˆ‚a)(âˆ‚a/âˆ‚z)(âˆ‚z/âˆ‚w)
\`\`\`

For sigmoid activation:

\`\`\`
Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))
\`\`\`

Error term:

\`\`\`
Î´ = (Å· - y) Ã— Ïƒ'(z)
\`\`\`

---

### ðŸ”¹ Step 4: Weight Update

Weights are updated using gradient descent:

\`\`\`
w_new = w_old - Î·(âˆ‚L/âˆ‚w)
\`\`\`

Where:
- **Î·** = learning rate  

---

### ðŸ”¹ Summary

Backpropagation:
- Computes prediction error  
- Propagates error backward through layers  
- Updates weights iteratively  
- Reduces loss efficiently  

It is the fundamental optimization algorithm behind deep learning.
`



// import OpenAI from "openai";

// const client = new OpenAI({
//   apiKey: process.env.OPENAI_API_KEY
// });

// export const callOpenAI = async (message) => {
//   const response = await client.chat.completions.create({
//     model: "gpt-4o-mini",
//     messages: [
//       { role: "user", content: message }
//     ]
//   });

//   return response.choices[0].message.content;
// };